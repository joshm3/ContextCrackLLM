# Modified version of passgpt-16chars.yaml from PassGPT at https://github.com/javirandor/passgpt/blob/main/configs/passgpt-16chars.yaml
# Adds the model_path to fine tune a previously trained model rather than randomly initializing one
# Mean to be used with tune tune_passgpt.py

# Execution-wide parameters 
config_args:
    seed: 14
    maxchars: 16 # Maximum characters to be considered in your passwords
    subsample: -1 # -1 means no subsampling training data
    tokenizer_path: '' # Introdue the path or huggingface name for your tokenizer
    train_data_path: '' # Path to your training data
    model_path: ''

# Details for model architecture. Set parameters directly for GPT2Config (https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Config)
model_args:
    n_head: 12
    n_layer: 8

# Set parameters directly for TrainingArguments (https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)
training_args:
    per_device_train_batch_size: 1024
    gradient_accumulation_steps: 1
    logging_steps: 250
    save_total_limit: 1
    num_train_epochs: 3
    overwrite_output_dir: true
    fp16: false
    output_dir: '' # Where to store your checkpoints
    report_to: "wandb"
    save_steps: 50000
    
